{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### CartPole\n",
    "##### with Q-learning from scratch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71bde2b1ca7b6f53"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.animation\n",
    "import threading\n",
    "import pickle\n",
    "import joblib\n",
    "import copy\n",
    "import threading\n",
    "from sklearn import impute, preprocessing, model_selection, base, metrics, linear_model, pipeline, ensemble, svm, multiclass, neighbors, compose, datasets, decomposition, manifold\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from scratch_models import my_decorators\n",
    "import gym\n",
    "import pygame as pg"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.826631Z",
     "start_time": "2023-10-10T05:50:09.226689400Z"
    }
   },
   "id": "fd0fb9d0358ad7e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Environment\n",
    "**Setup the CartPole environment**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37564b75be5662c0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "clock = pg.time.Clock()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.851904400Z",
     "start_time": "2023-10-10T05:50:18.818940700Z"
    }
   },
   "id": "10696c005d35255e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up initial parameters for Q-learning:\n",
    "\n",
    "**$\\alpha$ = 0.075** is the learning rate;\n",
    "**$\\epsilon$ = 1.0** is the initial value that will be used in Exploration-Exploitation trade-off for $\\epsilon$-greedy algorithm;\n",
    "**$\\gamma$ = 0.95** is the discount factor, the higher the $\\gamma$, the more the weightage of future states on the returns\n",
    "\n",
    "**Q-table** : Initially a 25x25x25x25x2 tensor with zeroes, because we will be testing 25 different values for each of the 4 states against 2 possible actions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e378e253214ee8a7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set initial parameters\n",
    "params = {'alpha':0.075, 'gamma':0.95, 'epsilon':1.0}\n",
    "\n",
    "# Initialize Q-table\n",
    "n_actions = env.action_space.n\n",
    "state_bins = [np.linspace(-x, x, num=24) for x in env.observation_space.high]\n",
    "Q_table = np.zeros([len(bin) + 1 for bin in state_bins] + [n_actions])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.888972500Z",
     "start_time": "2023-10-10T05:50:18.849818600Z"
    }
   },
   "id": "32c8eb8d26d25ed7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Define a Policy**\n",
    "I am going to explore three different policies:\n",
    "   * User Controlled (``rl=no-policy``) : User plays the game with keyboard inputs\n",
    "    * Harcoded policy (``rl='basic'``) : A simple rule based policy that moves the cart to right if the pole tilts right and moves the cart left if the pole tilts left, in order to balance the pole.\n",
    "   * Q-learning based policy (``rl='q-learning'``) : Applies **Q-learning** to learn by interacting with the environment on it's own."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206386f05e3775ae"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def policy(obs, rl='no-policy'):\n",
    "    action = None\n",
    "    \n",
    "    if rl == 'no-policy':\n",
    "        return action\n",
    "\n",
    "    if rl=='basic':\n",
    "        angle = obs[2]\n",
    "        action = 1 if angle>0 else 0\n",
    "        return action\n",
    "    \n",
    "    if rl=='q-learning':            \n",
    "        # Choose action as per epsilon-greedy method\n",
    "        if np.random.rand()<params['epsilon']:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[obs])\n",
    "        if params['epsilon']>0.1:\n",
    "            params['epsilon']*=0.995\n",
    "        return action   "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.890011700Z",
     "start_time": "2023-10-10T05:50:18.866857500Z"
    }
   },
   "id": "30138ad94802a5ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Update the Q-table**\n",
    "This step is the essence of Q-learning, the Q-table initialized above gets updated based next state, action and reward as per the following expression:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow (1 - \\alpha) \\cdot Q(s, a) + \\alpha \\cdot (r + \\gamma \\cdot \\max_{a'} Q(s', a'))\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "806ae9d675dce314"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Update Q-table\n",
    "def update_Q(obs, action, reward):\n",
    "    best_next_action = np.argmax(Q_table[obs])\n",
    "    Q_table[obs][action] = (1 - params['alpha']) * Q_table[obs][action] + params['alpha'] * (reward + params['gamma'] * Q_table[obs][best_next_action])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.891063500Z",
     "start_time": "2023-10-10T05:50:18.880359600Z"
    }
   },
   "id": "c7bda4a98064244b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following function is simply to discretize the state space returned by CartPole so that it's compatible with the Q-table."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f67afa2714a5434"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Will be used to discretize the state space as CartPole returns a continuous space \n",
    "def discretize(state):\n",
    "    state_indices = []\n",
    "    for i in range(len(state)):\n",
    "        lower_bound = state_bins[i][0] if isinstance(state_bins[i][0], (int, float)) else state_bins[i][0][0]\n",
    "        upper_bound = state_bins[i][-1] if isinstance(state_bins[i][-1], (int, float)) else state_bins[i][-1][-1]\n",
    "\n",
    "        if state[i] <= lower_bound:\n",
    "            state_index = 0\n",
    "        elif state[i] >= upper_bound:\n",
    "            state_index = len(state_bins[i]) - 1\n",
    "        else:\n",
    "            state_index = np.digitize(state[i], state_bins[i]) - 1\n",
    "        state_indices.append(state_index)\n",
    "\n",
    "\n",
    "    return tuple(state_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.912825400Z",
     "start_time": "2023-10-10T05:50:18.897024200Z"
    }
   },
   "id": "15dee5f717e044ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Draw the Game Screen**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96ca586d17805c95"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Run animations\n",
    "def animate(env, win, clock, episode_rewards, env_rewards, policy_type):\n",
    "    clock.tick(15)\n",
    "    run = True\n",
    "    frame = env.render()\n",
    "    frame = np.rot90(frame)\n",
    "    frame = np.flipud(frame)\n",
    "    \n",
    "    font = pg.font.Font(None, 36)\n",
    "    text_surface_current = font.render('Current Score: '+ str(episode_rewards), True, (100,0,0))\n",
    "    text_surface_high = font.render('Highest Score: '+str(max(env_rewards[policy_type])), True, (0,100,100))\n",
    "    surface = pg.surfarray.make_surface(frame)\n",
    "    \n",
    "    win.blit(surface, (0, 0))\n",
    "    win.blit(text_surface_current, (0, 0))\n",
    "    win.blit(text_surface_high, (0, 40))\n",
    "    pg.display.update()\n",
    "\n",
    "    for event in pg.event.get():\n",
    "        if event.type == pg.QUIT:\n",
    "            run = False\n",
    "            pg.quit()\n",
    "    return run\n",
    "            \n",
    "# User controlled settings if no policy     \n",
    "def run_by_user(animation):\n",
    "    action = None\n",
    "    if animation:\n",
    "        user_input = pg.key.get_pressed()\n",
    "        if user_input[pg.K_RIGHT]:\n",
    "            action = 1\n",
    "        elif user_input[pg.K_LEFT]:\n",
    "            action = 0\n",
    "        return action\n",
    "    else:\n",
    "        raise ValueError('Animation must be True in user-controlled policy')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.964734200Z",
     "start_time": "2023-10-10T05:50:18.913962300Z"
    }
   },
   "id": "a3afdfdbca969f54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Simulate**\n",
    "Next, I shall create some functions to simulate the CartPole environment based on different policies. The first two policies are self-explanatory, while the Q-learning policy simply starts by taking an action based on the $\\epsilon$-greedy algorigthm, and then updates the Q-values until it reaches satisfactory score or is halted."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d046d0b5bdc5f63c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# List that holds rewards for each run\n",
    "env_rewards = {'no-policy':[0], 'basic':[0], 'q-learning':[0]}\n",
    "\n",
    "# Main function to run the simulation\n",
    "def simulate_episode(env, policy_type='no-policy', animation=True):\n",
    "    obs, info = env.reset()\n",
    "    obs = discretize(obs)\n",
    "    clock = pg.time.Clock()\n",
    "    \n",
    "    # Initialize Pygame if animation is True\n",
    "    if animation:\n",
    "        clock.tick(15)\n",
    "        win = pg.display.set_mode((600, 400))\n",
    "        pg.init()\n",
    "        run = True\n",
    "    else:\n",
    "        run = 500\n",
    "\n",
    "\n",
    "    episode_rewards = 0\n",
    "    while run:        \n",
    "        action=policy(obs, policy_type)\n",
    "        \n",
    "        # For User controlled setting\n",
    "        if action is None:\n",
    "            action = run_by_user(animation)\n",
    "                \n",
    "        if action is not None:\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            episode_rewards += reward\n",
    "            if policy_type=='q-learning':\n",
    "                obs = discretize(obs)\n",
    "                update_Q(obs, action, reward)\n",
    "            if done:                                                # COMMENT THIS SECTION OUT DURING PRESENTATION\n",
    "                obs, info = env.reset()\n",
    "                break\n",
    "        \n",
    "        if animation:\n",
    "            run = animate(env, win, clock, episode_rewards, env_rewards, policy_type)\n",
    "        else:\n",
    "            run-=1\n",
    "    \n",
    "    env_rewards[policy_type].append(episode_rewards)\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:50:18.964734200Z",
     "start_time": "2023-10-10T05:50:18.935302700Z"
    }
   },
   "id": "6bb918814a2cdba8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. User Controlled**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c81dd3a067b1787"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "29.0"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate_episode(env, policy_type='no-policy', animation=True)\n",
    "max(env_rewards['no-policy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:51:23.771400500Z",
     "start_time": "2023-10-10T05:51:17.689204200Z"
    }
   },
   "id": "4c108dfe74b352f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Basic Hardcoded Policy**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5809e3bb6b6f783"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "56.0"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for episode in range(20):    \n",
    "    simulate_episode(env, policy_type='basic', animation=True)\n",
    "    \n",
    "max(env_rewards['basic'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:53:01.097478600Z",
     "start_time": "2023-10-10T05:52:57.152395700Z"
    }
   },
   "id": "ec4e10507eee5c61"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Q-Learning**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da6a7f0fc87e6f81"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "24.0"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for episode in range(20):\n",
    "    simulate_episode(env, policy_type='q-learning', animation=True)\n",
    "max(env_rewards['q-learning'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-10T05:53:44.197037300Z",
     "start_time": "2023-10-10T05:53:42.353709900Z"
    }
   },
   "id": "9378dce1f5896160"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Kartik Kumar**\n",
    "For more projects, please visit [My GitHub Page](https://github.com/ryuukkk?tab=repositories)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba0a81d2e36da883"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
