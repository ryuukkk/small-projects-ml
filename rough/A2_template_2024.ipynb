{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCtvl5uf2Fg0"
   },
   "source": [
    "# Assignment 2: Classification\n",
    "# Using Machine Learning Tools\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this assignment, you will apply some popular machine learning techniques to the problem of classifying data from histological cell images for the diagnosis of malignant breast cancer. This will be presented as a practical scenario where you are approached by a client to solve a problem.  \n",
    "\n",
    "The main aims of this assignment are:\n",
    "\n",
    "- to use the best practice machine learning workflow for producing a solution to a client's problem;\n",
    "- to visualise and clean data;\n",
    "- to train and optimise a selection of models, then choose the best;\n",
    "- to obtain an unbiased measurement of the final model's performance;\n",
    "- to interpret results clearly and concisely.\n",
    "\n",
    "This assignment relates to the following ACS CBOK areas: abstraction, design, hardware and software, data and information, HCI and programming.\n",
    "\n",
    "## General instructions\n",
    "\n",
    "This assignment is divided into several tasks. Use the spaces provided in this notebook to answer the questions posed in each task, but feel free to add additional cells to structure your notebook. Note that some questions require writing code, some require graphical results, and some require comments or analysis as text. It is your responsibility to make sure your responses are clearly labelled and your code has been fully executed (**with the correct results displayed**) before submission!\n",
    "\n",
    "**Do not** manually edit the data set file we have provided! For marking purposes, it's important that your code can run correctly on the original data file.\n",
    "\n",
    "This assignment uses the standard best practice machine learning workflow, building on the first assignment and course workshops, and so less detailed instructions are provided for this assignment. You should be able to implement this workflow now without low-level guidance and a substantial portion of the marks for this assignment are associated with the appropriate choices and executing this workflow correctly and efficiently. Make sure you have clean, readable code as well as producing outputs, since your coding will also count towards the marks (however, excessive commenting is discouraged and will lose marks, so aim for a modest, well-chosen amount of comments and text in outputs).\n",
    "\n",
    "This assignment can be solved using methods from [sklearn](https://scikit-learn.org/stable/index.html), [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html), and [matplotlib](https://matplotlib.org/stable/index.html) as presented in the workshops. Other libraries should not be used (even though they might have nice functionality) and occasionally certain specific functions need to be used, which will be made clear in the instruction text. You are expected to make sure that you are using functions correctly, and you should search and carefully read the documentation if unsure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WAEeyxM2Fg1"
   },
   "source": [
    "# Scenario\n",
    "\n",
    "A client approaches you to solve a machine learning problem for them.  They run a pathology lab that processes histological images for healthcare providers and they have created a product that measures most of the same features as in the *Wisconsin breast cancer data set* though using different acquisitions and processing methods. Their method employs efficient stochastic sampling, making it much faster than existing methods, although slightly noisier. This method does not measure any of the 'worst' features that appear in the *Wisconsin Breast Cancer Data Set*. They want to be able to diagnose *malignant* cancer (and distinguish them from *benign* growths) by employing machine learning techniques, and they have asked you to implement this for them.\n",
    "\n",
    "Their requirements are:\n",
    " - 1) Have at least a 90% probability of detecting malignant cancer when it is present;\n",
    " - 2) Ensure that no more than 1 in 5 healthy cases (those with benign growths) result in a false positive (labeled as malignant).\n",
    "\n",
    "They have hand-labelled 220 samples for you, with 20 features per sample, which is all they have at the moment.\n",
    "\n",
    "Please follow the instructions below, which will vary in level of detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuW9d7F32Fg1"
   },
   "source": [
    "## 1. Load data, visualise it and identify erroneous values [30%]\n",
    "\n",
    " - Load the data from the csv file `assignment2_data_2024.csv` (found on MyUni).\n",
    " - Extract the feature names and label names for use later on.\n",
    " - Provide at least one text summary of the dataset. This should include key information and characteristics of the data.\n",
    " - Create one graphical plot per feature. Each plot must display the feature values for the two classes (malignant and benign) separately on a single axis/panel.\n",
    " - Make sure you clearly label the graphical plots with respect to feature names, axes, classes, etc.\n",
    " - Identify any erroneous values in the dataset during the data visualisation or summary steps. Determine and implement, here or later, the appropriate action to handle these values (e.g., removal, correction, or none).\n",
    " - Throughout this assignment, treat the _malignant_ cases as the _true positive_ class, following the standard convention in medicine.\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQAZlg-a2Fg2"
   },
   "outputs": [],
   "source": [
    "# This code imports some libraries that you will need.\n",
    "# You should not need to modify it, though you are expected to make other imports later in your code.\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Pandas for overview\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Plot setup\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=7)\n",
    "mpl.rc('xtick', labelsize=6)\n",
    "mpl.rc('ytick', labelsize=6)\n",
    "mpl.rc('figure', dpi=240)\n",
    "plt.close('all')\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33N0r8Gg2Fg2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swVFp5e42Fg3"
   },
   "source": [
    "## 2. Prepare data and models, and make baseline measurements [20%]\n",
    " - Split data into appropriate sets using the `stratify` option to obtain consistent proportions of classes in each set.\n",
    " - Decide on appropriate pre-processing steps for the data.\n",
    " - Implement a _very_ simple baseline model that makes purely random predictions of the class.\n",
    " - Measure the baseline performance with the following metrics:\n",
    "   - accuracy\n",
    "   - balanced accuracy: accounts for imblanaced datasets, defined as _(sensitivity + specificity)/2_\n",
    "   - recall\n",
    "   - precision\n",
    "   - auc\n",
    "   - f1score\n",
    "   - fbeta_scores with beta=0.1\n",
    "   - fbeta_score with beta=10\n",
    " - Also display a confusion matrix for the baseline predictions.\n",
    " - As a second baseline, implement an SGD classifier and fit it once (without hyper-parameter optimisation) and then display the same performance metrics and a confusion matrix for its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPIxf32x2Fg3"
   },
   "outputs": [],
   "source": [
    "# Some helpful code that you can use if you wish (or not)\n",
    "\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "f10_scorer = make_scorer(fbeta_score, beta=10)\n",
    "f01_scorer = make_scorer(fbeta_score, beta=0.1)\n",
    "\n",
    "def f10_score(yt,yp):\n",
    "    return fbeta_score(yt, yp, beta=10)\n",
    "\n",
    "def f01_score(yt,yp):\n",
    "    return fbeta_score(yt, yp, beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "desznVDe2Fg3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abqPbZBI2Fg3"
   },
   "source": [
    "## 3. Model Optimisation [40%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgzQG4Ue2Fg3",
    "tags": []
   },
   "source": [
    "### 3.1 Performance metric\n",
    "Choose one performance metric from the above set. State your choice and explain why you chose it. [50 words maximum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfdnAsS62Fg3"
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rteHJbL72Fg3"
   },
   "source": [
    "### 3.2 Hyper-parameter optimisation\n",
    " - Perform a hyper-parameter optimisation (using appropriate methods) on three models:\n",
    "   - SGD Classifer\n",
    "   - SVM\n",
    "   - One other model of your choice\n",
    " - Display the results of each model (including confusion matrices) and choose the best model.\n",
    " - Choose the best model and display the final results of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gurClAG02Fg4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUpt7i0M2Fg4"
   },
   "source": [
    "### 3.3 Final results\n",
    "\n",
    "From the final results calculate the _probability_ that a sample from a person with a malignant tumour is given a result that they do not have cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwuWNj-32Fg4"
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUFJv7Zs2Fg4"
   },
   "source": [
    "## 4. Decision Boundaries [10%]\n",
    " - To finish up, the client wants a visualisation of how the final model is working and your line manager has asked you to show some of the decision boundaries.\n",
    " - The client also wants to know if your method has met their performance specifications.\n",
    " - Follow the next three steps (4.1, 4.2 and 4.3) to do these things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2YJjphH2Fg4",
    "tags": []
   },
   "source": [
    "### 4.1 Discriminative features\n",
    "\n",
    "Although it is only possible to know the true usefulness of a feature when you've combined it with others in a machine learning method, it is still helpful to have some measure for how discriminative each feature is on its own.  One common method for doing this is to calculate a T-score (often used in statistics, and in the LDA machine learning method) for each feature.  \n",
    "\n",
    "The formula for the T-score is:\n",
    "\n",
    "_(mean(x2) - mean(x1))/(0.5*(stddev(x2) + stddev(x1)))_\n",
    "\n",
    "where x1 and x2 are the feature values corresponding to the two classes. Large values for the T-score (either positive or negative) indicate discriminative ability.\n",
    "\n",
    "**Calculate the T-score for each feature and print out the best 4 features according to this score.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoboVned2Fg4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt1x19oE2Fg4"
   },
   "source": [
    "### 4.2 Visualise decision boundaries\n",
    "\n",
    "**Display the decision boundaries** for each pair of features from the best 4 chosen above.  You can use the functions below to help if you like.\n",
    "\n",
    "Instead of using the simple mean as the input for `xmean` in `plot_contours`, use the following:\n",
    "\n",
    "_0.5*(mean(x1) + mean(x2))_\n",
    "\n",
    "where x1 and x2 are the data associated with the two classes.  This way of calculating a \"mean\" point takes into account any imbalance between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBDGgN1s2Fg4"
   },
   "outputs": [],
   "source": [
    "def make_meshgrid(x, y, ns=100):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on (only min and max used)\n",
    "    y: data to base y-axis meshgrid on (only min and max used)\n",
    "    ns: number of steps in grid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    hx = (x_max - x_min)/ns\n",
    "    hy = (y_max - y_min)/ns\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max + hx, hx), np.arange(y_min, y_max + hy, hy))\n",
    "    return xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OlPfyxT62Fg4"
   },
   "outputs": [],
   "source": [
    "def plot_contours(clf, xx, yy, xmean, n1, n2, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    xmean : 1d array of N mean values (used to populate constant features with)\n",
    "    n1, n2: index numbers of features that change\n",
    "              that is, which features xx and yy represent, from the set of N features\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    # The following lines makes an MxN matrix to pass to the classifier (# samples x # features)\n",
    "    # It does this by multiplying Mx1 and 1xN matrices, where the former is filled with 1's\n",
    "    #  where M is the number of grid points in xx and N is the number of features in xmean\n",
    "    #  It is done in such a way that the xmean vector is replaced in each row\n",
    "    fullx = np.ones((xx.ravel().shape[0],1)) * np.reshape(xmean,(1,-1))\n",
    "    fullx[:,n1] = xx.ravel()\n",
    "    fullx[:,n2] = yy.ravel()\n",
    "    Z = clf.predict(fullx)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = plt.contourf(xx, yy, Z, **params)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iOUnCzpu2Fg5"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuSF1Cq92Fg5"
   },
   "source": [
    "### 4.3 Performance specification\n",
    "**Does the final model meet the client's criteria?  Explain why or why not.** [100 words maximum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TXQS1WS2Fg5"
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
