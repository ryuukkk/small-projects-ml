{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50036eab",
   "metadata": {},
   "source": [
    "# Assignment 2 Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1ead5",
   "metadata": {},
   "source": [
    "## Question 1a: SVM Primal Form\n",
    "\n",
    "Please implement the training and testing algorithms of soft-margin Linear Support Vector Machine in its primal form, that is,\n",
    "\n",
    "$$\\min_{\\mathbf{w},b,\\{\\xi_i\\}} \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C\\sum_{i=1}^N \\xi_i \\nonumber \\\\ s.t.~~ y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1 - \\xi_i, ~~\\forall i \\nonumber \\\\ \\xi_i \\ge 0 \\nonumber$$\n",
    "Use CVXPY in your implementation strictly following the format given in this Notebook, and supplying missing code **in the indicated space. Do not add or change any other code**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14fe37d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "!pip install cvxpy --upgrade # if needed\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb4d7723",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:1500].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:1500].iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9af991c",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# get test dataset\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:500, 1:].to_numpy()\n",
    "Y_test = df.iloc[:500, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a355188",
   "metadata": {
    "id": "QhiZQBI0Fhy3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of W, b and slack:\n",
      "0.46\n"
     ]
    }
   ],
   "source": [
    "# train linear svm in primal form\n",
    "def svm_train_primal(data_train, label_train, C):\n",
    "    X, Y = data_train, label_train\n",
    "    n_samples, m_features = np.shape(X)\n",
    "    \n",
    "    W_value = 0\n",
    "    b_value = 0\n",
    "    slack_var_value = 0\n",
    "\n",
    "    W = cp.Variable(m_features)\n",
    "    b = cp.Variable()\n",
    "    slack_var = cp.Variable(n_samples)\n",
    "    \n",
    "    objective = cp.Minimize(0.5 * cp.sum_squares(W) + C * cp.sum(slack_var))\n",
    "    constraints = [cp.multiply(Y, (X @ W + b)) >= 1 - slack_var, slack_var >= 0]\n",
    "    \n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve()\n",
    "    \n",
    "    W_value = W.value\n",
    "    b_value = b.value\n",
    "    slack_var_value = slack_var.value\n",
    "\n",
    "    return [W_value, b_value, slack_var_value]\n",
    "\n",
    "# train primal model\n",
    "C = 1\n",
    "model_primal = svm_train_primal(X_train, Y_train, C)\n",
    "\n",
    "# output svm primal form solutions\n",
    "W = model_primal[0]\n",
    "b = model_primal[1]\n",
    "slack = model_primal[2]\n",
    "\n",
    "print('Sum of W, b and slack:')\n",
    "print(np.round(np.sum(W)+np.sum(slack)+b,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd871c",
   "metadata": {},
   "source": [
    "## Question 1b: SVM Primal Accuracy\n",
    "\n",
    "Please complete and run this code and copy the result into Assignment 2 Question 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde72cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.94\n"
     ]
    }
   ],
   "source": [
    "# predict accuracy of svm model on test dataset\n",
    "def svm_predict(data_test, label_test, svm_model):\n",
    "    \n",
    "    acc = 0.0\n",
    "    \n",
    "    W_value, b_value, slack_var_value = svm_model\n",
    "    \n",
    "    predictions = np.sign(np.dot(data_test, W_value) + b_value)\n",
    "    correct_predictions = np.sum(predictions == label_test)\n",
    "    acc = correct_predictions / len(label_test)\n",
    "\n",
    "    return acc\n",
    "\n",
    "# predict and output primal accuracy\n",
    "accuracy = svm_predict(X_test, Y_test, model_primal)\n",
    "print('Accuracy:')\n",
    "print(np.round(accuracy,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130676a2",
   "metadata": {},
   "source": [
    "## Question 2a: SVM Dual Form\n",
    "\n",
    "Please implement the training and testing algorithms of soft-margin Linear Support Vector Machine in its **dual** form, that is,\n",
    "\n",
    "$$\\max_{\\alpha_i}\\sum_i \\alpha_i - \\frac{1}{2}\\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j <\\mathbf{x}_i, \\mathbf{x}_j> \\nonumber \\\\ s.t. ~~~ 0 \\le \\alpha_i \\le C\\nonumber \\\\ ~~~ \\sum_i \\alpha_i y_i = 0 \\nonumber$$\n",
    "\n",
    "Use CVXPY in your implementation strictly following the format given in this Notebook, and supplying missing code in the indicated space. **Do not modify any other code**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c723a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import cvxpy as cp\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760d985",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df[:1500].iloc[:, 1:].to_numpy()\n",
    "Y_train = df[:1500].iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8287b42",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# get test dataset\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:500, 1:].to_numpy()\n",
    "Y_test = df.iloc[:500, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train linear svm in dual form\n",
    "def svm_train_dual(data_train, label_train, C):\n",
    "    \n",
    "    alphas = 0\n",
    "    x, y = data_train, label_train\n",
    "    n_samples, n_features = np.shape(x)\n",
    "    \n",
    "\n",
    "    K = np.dot(x, x.T)  # Compute the kernel (linear kernel)\n",
    "    alpha = cp.Variable(n_samples)\n",
    "    \n",
    "    # Ensure the kernel matrix is symmetric\n",
    "    P = cp.multiply(y[:, None] * y[None, :], K)\n",
    "    P_sym = 0.5 * (P + P.T)  # Make sure P is symmetric\n",
    "    \n",
    "    objective = cp.Maximize(cp.sum(alpha) - 0.5 * cp.quad_form(alpha, P_sym))\n",
    "    constraints = [alpha >= 0, alpha <= C, cp.sum(cp.multiply(alpha, y)) == 0]\n",
    "    \n",
    "    problem = cp.Problem(objective, constraints)\n",
    "    problem.solve(solver=cp.SCS)\n",
    "    \n",
    "    alphas = alpha.value\n",
    "\n",
    "\n",
    "    # return svm dual model alphas\n",
    "    return alphas\n",
    "\n",
    "# train dual model\n",
    "c = 1\n",
    "alphas = svm_train_dual(X_train, Y_train, c)\n",
    "\n",
    "# output svm dual form solutions\n",
    "print('Sum of alphas:')\n",
    "print(np.round(np.sum(alphas),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3fe63",
   "metadata": {},
   "source": [
    "## Question 2b: Dual model parameters\n",
    "\n",
    "Complete the code where indicated, run it and copy the result into Assignment 2 Question 2b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03e20c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# obtain primal w*, b* from dual solution\n",
    "def find_model_params_from_dual(data, label, alphas, C):\n",
    "    \n",
    "    # this value is used to compare values generated by CVXPYY to zero.\n",
    "    # for example, instead of \n",
    "    # if var > 0, we use: if var > zero_threshold\n",
    "    zero_threshold = 0.0001\n",
    "    n_samples, n_features = np.shape(data)\n",
    "    a = alphas\n",
    "   \n",
    "    a[np.isclose(a, 0, atol=zero_threshold)] = 0  # zero out nearly zeros\n",
    "    a[np.isclose(a, C, atol=zero_threshold)] = C  # round the ones that are nearly C\n",
    "\n",
    "\n",
    "    # b is scalar, but may be sliglty diffrent for each Y, calculate b for each label and return mean of them\n",
    "# ====================== YOUR CODE HERE ====================== \n",
    "    w_dual = np.sum(a[:, None] * label[:, None] * data, axis=0)\n",
    "\n",
    "    # Find support vectors (indices where 0 < a < C)\n",
    "    support_vector_indices = np.where((a > zero_threshold) & (a < C))[0]\n",
    "\n",
    "    b_dual_values = []\n",
    "    for i in support_vector_indices:\n",
    "        b_dual_i = label[i] - np.dot(w_dual, data[i])\n",
    "        b_dual_values.append(b_dual_i)\n",
    "\n",
    "    b_dual = np.mean(b_dual_values)\n",
    " \n",
    "# DO NOT use any other import statements for this question\n",
    "# Note: b_dual is a scalar, but maybe different for each label, therefore calculate the mean\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "    return w_dual, b_dual\n",
    "\n",
    "# output reconstructed w* and b* from svm dual problem\n",
    "C = 1\n",
    "model_dual = find_model_params_from_dual(X_train, Y_train, alphas, C)\n",
    "\n",
    "print('Sum of W and b:')\n",
    "print(np.round(np.sum(model_dual[0])+model_dual[1],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83ea70-ae9e-4baf-8835-6833f177da60",
   "metadata": {},
   "source": [
    "## Question 3a: PCA implementation\n",
    "\n",
    "1. Perform PCA on the dataset to reduce each sample into a 10-dimensional feature vector.\n",
    "2. Print the required result and enter into Assignment 2 Question 3a.\n",
    "\n",
    "=========================================================================\n",
    "- Implementing PCA algorithm.\n",
    "    - Start\n",
    "        - Input: number of samples as matrix $X$ of $m$ rows and $k$ columns.\n",
    "        - Calculate the mean for each column. $$mean = \\frac {1}{m} \\sum \\limits _{i=1} ^{n}X_{ij}$$\n",
    "        - Calculate the centralised matrix $X_C$ and covariance matrix $C$. $$X_C=X-mean$$ $$C = \\frac {1}{m}(X_C)^TX_C$$\n",
    "        - Calculate the eigenvalues and eigenvectors using convariance matrix.\n",
    "        - Select top x principal components - which are eigen vector corresponding to top x eigen values. Construct matrix $P$.\n",
    "    - End\n",
    "    \n",
    "- Transforming the the data using the principal components (matrix $P$) obtained using the PCA algorithm. $$Transformed \\: Data = XP$$\n",
    "- Calculating the covariance matrix of the transformed data by first centralising it(mean subtracted) and then obtaining the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32c40204",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aed4563b",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X = df[:1500].iloc[:, 1:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55475a58-cd32-4c7f-8263-ffad66c91dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_cov_X_transformed:\n",
      "314.96\n"
     ]
    }
   ],
   "source": [
    "# Selecting top 10 Principal components\n",
    "no_of_components = 10\n",
    "\n",
    "covariance_matrix_X = 0\n",
    "covariance_matrix_X_transformed = 0\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# Centralize the data\n",
    "X_mean = np.mean(X, axis=0)\n",
    "X_centered = X - X_mean\n",
    "\n",
    "# Compute the covariance matrix\n",
    "covariance_matrix_X = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "# Calculate eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(covariance_matrix_X)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Select the top 10 eigenvectors (principal components)\n",
    "P = sorted_eigenvectors[:, :no_of_components]\n",
    "\n",
    "# Transform the data using the principal components\n",
    "X_transformed = np.dot(X_centered, P)\n",
    "\n",
    "# Compute the covariance matrix of the transformed data\n",
    "X_transformed_centered = X_transformed - np.mean(X_transformed, axis=0)\n",
    "covariance_matrix_X_transformed = np.cov(X_transformed_centered, rowvar=False)\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================================\n",
    "\n",
    "sum_cov_X = np.sum(covariance_matrix_X)\n",
    "sum_cov_X_transformed = np.sum(covariance_matrix_X_transformed)\n",
    "\n",
    "print(\"sum_cov_X_transformed:\")\n",
    "print(np.round(sum_cov_X + sum_cov_X_transformed,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e300ef44",
   "metadata": {},
   "source": [
    "## Question 3b: PCA data reconstructon\n",
    "\n",
    "For this question:\n",
    "1. Reconstruct the X dataset from your results in code Question 3a as X_back.\n",
    "1. Calculate covariance matrix for X_back and enter its sum into Assignment 2 Question 3b.\n",
    "\n",
    "For this part, use the libraries imported in Question 3a, and do not import any moore libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d53eb57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of covariance_matrix_X_back:\n",
      "58.4\n"
     ]
    }
   ],
   "source": [
    "# Do not import any additional libraries for this section\n",
    "# Do not change any code outside of this area marked with =============================\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "# Reconstruct the original dataset from the transformed data\n",
    "X_back = np.dot(X_transformed, P.T) + X_mean\n",
    "\n",
    "# Compute the covariance matrix of the reconstructed data\n",
    "X_back_centered = X_back - np.mean(X_back, axis=0)\n",
    "covariance_matrix_X_back = np.cov(X_back_centered, rowvar=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "print(\"Sum of covariance_matrix_X_back:\")\n",
    "print(np.round(np.sum(covariance_matrix_X_back),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02272d",
   "metadata": {},
   "source": [
    "## Questions 4a and 4b: Kernel k-Means derivation\n",
    "\n",
    "Complete these questions in myUni assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a331268",
   "metadata": {},
   "source": [
    "## Question 4c: Kernel k-Means implemention\n",
    "\n",
    "\n",
    "In Question 4a and 4b you have proved the relationship between the variance of distances $$m_1 = \\frac{1}{m} \\sum_i\\|\\mathbf{x}_i - \\mathbf{\\mu}\\|_2^2$$ and average of squared pairwise distances $$m_2= \\frac{1}{m^2}\\sum_i\\sum_j \\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2$$\n",
    "\n",
    "In this implementation, you will use this relationship.\n",
    "\n",
    "In this question you will implement kernel k-means with modified RBF-kernel, using SpectralClustering from sklearn to reduce the programming efford of the full implementation. \n",
    "RBF-kernel is as follows:\n",
    "\n",
    "$$(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2m_1})$$\n",
    "\n",
    "The modification of RBF kernel will be that the $m_1$ will be replaced with equivalent form of $m_2$ as you proved in Question 4a and selected in 4b. For example, if you proved that $m_1^2=m_2$, then you will implement\n",
    "\n",
    "$$(\\mathbf{x}_i,\\mathbf{x}_j) = \\exp(\\frac{-\\|\\mathbf{x}_i - \\mathbf{x}_j\\|_2^2}{2\\sqrt{m_2}})$$\n",
    "\n",
    "\n",
    "For reference, please write the formula of your RBF kernel as\n",
    "$$(\\mathbf{x}_i,\\mathbf{x}_j)=...your formula...$$\n",
    "\n",
    "\n",
    "In order to calculate pairwise distance matrix, please use scipy.spatial distance function, which is alread pre-loaded in the code. <br>\n",
    "Please check the documentation of the SpectralClustering function on how to use precomputed affinity matrix, which you are going to supply with the above specification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "309b239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from scipy.spatial import distance\n",
    "\n",
    "df = pd.read_csv('train1.csv', header = None)\n",
    "X = df.iloc[:1000,1:].to_numpy(copy=True)\n",
    "Y = df.iloc[:1000,:1].to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "494c1d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of centroids:\n",
      "17.32\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10\n",
    "centroids_rbf = np.array(n_clusters)\n",
    "\n",
    "centroids_rbf = []\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================  \n",
    "# DO NOT use any other import statements for this question\n",
    "\"\"\"\n",
    "# use random_state=0 in the SpectralClustering function to make the results reproducible.\n",
    "\"\"\"\n",
    "# Calculate the pairwise distance matrix\n",
    "pairwise_dists = distance.cdist(X, X, 'sqeuclidean')\n",
    "\n",
    "# Calculate m2 (average of squared pairwise distances)\n",
    "m2 = np.mean(pairwise_dists)\n",
    "\n",
    "# Define the modified RBF kernel\n",
    "gamma = 1 / (2 * np.sqrt(m2))\n",
    "rbf_kernel = np.exp(-gamma * pairwise_dists)\n",
    "\n",
    "# Perform Spectral Clustering using the precomputed RBF kernel as affinity matrix\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, affinity='precomputed', random_state=0)\n",
    "clustering.fit(rbf_kernel)\n",
    "\n",
    "# Find the centroids of the clusters\n",
    "labels = clustering.labels_\n",
    "centroids_rbf = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "    \n",
    "print('Sum of centroids:')\n",
    "print(np.round(np.sum(centroids_rbf),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caf2e7",
   "metadata": {},
   "source": [
    "## Question 5a: Adaboost questions\n",
    "\n",
    "Answer questions in myUni assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b016ae9",
   "metadata": {},
   "source": [
    "## Question 5b: Adaboost code correction\n",
    "The following code shows an incomplete implementation of Adaboost algorithm and contains some errors. The code does not implement the logic specific to Adaboost. Please modify it to make a correct implementation. After completing the implementation, run the code and copy the result into space provided in myUni Question 6.\n",
    "\n",
    "**You can change only sections of the code as indicated.** <br>\n",
    "**Please clearly comment the purpose of the code that you added or changed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97e6dc33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtfdriY9Fhyw",
    "outputId": "adc09825-1942-4209-ab34-e4d6148a3d8e"
   },
   "outputs": [],
   "source": [
    "# Do not change this section\n",
    "\n",
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from math import exp, log as ln\n",
    "\n",
    "# DO NOT use any other import statements for this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94abf099",
   "metadata": {
    "id": "huRMLmS2IQjT"
   },
   "outputs": [],
   "source": [
    "# Do not change this section\n",
    "\n",
    "# get training dataset\n",
    "train = \"train1.csv\"\n",
    "df = pd.read_csv(train, header=None)\n",
    "X_train = df.iloc[:, 1:].to_numpy()\n",
    "Y_train = df.iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44d8ddc5",
   "metadata": {
    "id": "4G3CJIcaIogN"
   },
   "outputs": [],
   "source": [
    "# Do not change this section\n",
    "\n",
    "# get test dataset\n",
    "test = \"test1.csv\"\n",
    "df = pd.read_csv(test, header=None)\n",
    "X_test = df.iloc[:, 1:].to_numpy()\n",
    "Y_test = df.iloc[:, 0].replace(0, -1).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "055c51ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================= Do not change this section\n",
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "# sample_weights for training data\n",
    "def weak_classifier_train(train_data, train_label, sample_weights):\n",
    "    # Create a decision stump\n",
    "    stump = DecisionTreeClassifier(max_depth=1)\n",
    "    \n",
    "    # Train the stump using the weighted samples\n",
    "    stump.fit(train_data, train_label, sample_weight=sample_weights)\n",
    "    \n",
    "    # Generate predictions and calculate error\n",
    "    model_prediction = stump.predict(train_data)\n",
    "    train_label = np.array(train_label)\n",
    "    model_prediction = np.array(model_prediction)\n",
    "    sample_weights = np.array(sample_weights)\n",
    "\n",
    "    # The following line should work if both train_label and model_prediction are numpy arrays of the same size.\n",
    "    error = np.sum(sample_weights[train_label != model_prediction])\n",
    "    \n",
    "    return stump, error, model_prediction\n",
    "\n",
    "def weak_classifier_prediction(test_data, model):\n",
    "    \n",
    "    # The model_param_t is our trained stump, so we use it to make predictions\n",
    "    return model.predict([test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abec79",
   "metadata": {},
   "source": [
    "You can add or change the code in the following section **only where indicated.** <br>\n",
    "**Please clearly comment the purpose of the code that you added or changed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a282ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT use any other import statements for this question\n",
    "\n",
    "def Adaboost_train(train_data, train_label, T):\n",
    "    # train_data: N x d matrix\n",
    "    # train_label: N x 1 vector\n",
    "    # T: the number of weak classifiers in the ensemble\n",
    "\n",
    "    ensemble_models = []\n",
    "    alphas = []  # List of all alpha values\n",
    "\n",
    "    N = len(train_label)  # The number of samples\n",
    "\n",
    "    # Initialize weight distribution for each sample to be evenly distributed\n",
    "    D = np.array([1 / N] * N)\n",
    "\n",
    "    # For each weak classifier, do the following\n",
    "    for t in range(T):\n",
    "\n",
    "        # ====================== Add and/or correct code in this section ======================  \n",
    "        \n",
    "        # weak_classifier_train takes the training data and labels as well as the sample weight distribution\n",
    "        # returns the model parameters and the error\n",
    "        # model returns the model parameters of the learned weak classifier\n",
    "        model = weak_classifier_train(train_data, train_label, D)  \n",
    "\n",
    " \n",
    "        # ===============================================================================  \n",
    "\n",
    "        # definition of model\n",
    "        ensemble_models.append(model)\n",
    "\n",
    "    return ensemble_models, alphas\n",
    "\n",
    "\n",
    "def Adaboost_test(test_data, ensemble_models, alphas):\n",
    "    # test_data: n x d\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        decision_ensemble = 0\n",
    "\n",
    "        # ====================== Change or correct code in this section ======================  \n",
    " \n",
    "        # For each model in the ensemble models, do the following\n",
    "        for k in range(len(ensemble_models)):\n",
    "\n",
    "            # Make a prediction on the classification of the sample\n",
    "            prediction = weak_classifier_prediction(test_data[i], ensemble_models[k])  # prediction returns 1 or -1 prediction from the weak classifier\n",
    "\n",
    "            # Add this classification multiplied by its confidence (alpha) to the sum (either +1 or -1)\n",
    "            decision_ensemble += prediction\n",
    "\n",
    "        # If more models predicted it as +1 class\n",
    "        if decision_ensemble > 0:\n",
    "            prediction = 1\n",
    "        # Else, more models predicted it as -1 class\n",
    "        else:\n",
    "            prediction = -1\n",
    "        \n",
    "        # ============================================================================  \n",
    " \n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# predict and output accuracy\n",
    "\n",
    "ensemble_models, alphas = Adaboost_train(X_train, Y_train, 3)\n",
    "predicted_labels = Adaboost_test(X_test, ensemble_models, alphas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e91e5b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Adaboost training function\n",
    "def Adaboost_train(train_data, train_label, T):\n",
    "    # train_data: N x d matrix\n",
    "    # train_label: N x 1 vector\n",
    "    # T: the number of weak classifiers in the ensemble\n",
    "\n",
    "    ensemble_models = []\n",
    "    alphas = []  # List of all alpha values\n",
    "\n",
    "    N = len(train_label)  # The number of samples\n",
    "\n",
    "    # Initialize weight distribution for each sample to be evenly distributed\n",
    "    D = np.array([1 / N] * N)\n",
    "\n",
    "    # For each weak classifier, do the following\n",
    "    for t in range(T):\n",
    "\n",
    "        # Train weak classifier\n",
    "        stump, error, model_prediction = weak_classifier_train(train_data, train_label, D)  \n",
    "\n",
    "        # Compute alpha (model weight)\n",
    "        alpha = 0.5 * ln((1 - error) / error)\n",
    "\n",
    "        # Store the model and alpha\n",
    "        ensemble_models.append(stump)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "        # Update sample weights\n",
    "        D = D * np.exp(-alpha * train_label * model_prediction)\n",
    "        D = D / np.sum(D)  # Normalize to make it a distribution\n",
    "\n",
    "    return ensemble_models, alphas\n",
    "\n",
    "# Adaboost test function\n",
    "def Adaboost_test(test_data, ensemble_models, alphas):\n",
    "    # test_data: n x d\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        decision_ensemble = 0\n",
    "\n",
    "        # For each model in the ensemble models, do the following\n",
    "        for k in range(len(ensemble_models)):\n",
    "\n",
    "            # Make a prediction on the classification of the sample\n",
    "            prediction = ensemble_models[k].predict([test_data[i]])[0]  # prediction returns 1 or -1 prediction from the weak classifier\n",
    "\n",
    "            # Add this classification multiplied by its confidence (alpha) to the sum\n",
    "            decision_ensemble += alphas[k] * prediction\n",
    "\n",
    "        # If the sum of weighted predictions is positive, predict +1, otherwise predict -1\n",
    "        if decision_ensemble > 0:\n",
    "            prediction = 1\n",
    "        else:\n",
    "            prediction = -1\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# predict and output accuracy\n",
    "\n",
    "ensemble_models, alphas = Adaboost_train(X_train, Y_train, 3)\n",
    "predicted_labels = Adaboost_test(X_test, ensemble_models, alphas)\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "print('Accuracy:', np.round(accuracy, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7524ec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.983\n"
     ]
    }
   ],
   "source": [
    "######## do not change this code ############\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predicted_labels)\n",
    "\n",
    "print('Accuracy:')\n",
    "print(np.round(accuracy,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
